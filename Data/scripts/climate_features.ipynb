{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4e9b9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.interpolate import LinearNDInterpolator, NearestNDInterpolator\n",
    "from pyproj import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5242adcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ClimateConfig:\n",
    "    weather_csv: str                        # S-DoT measurements (with 'datetime' + 'serial' + features)\n",
    "    stations_csv: str                       # station metadata (SerialNum, Ylat, Xlon)\n",
    "    var_cols: List[str]                     # features to use (e.g., [\"temperature_mean_C\", \"humidity_mean_pc\"])\n",
    "    start_time_dt: pd.Timestamp             # align to movement start (tz-aware, e.g. Asia/Seoul)\n",
    "    bin_seconds: int                        # >= 3600\n",
    "    max_gap_minutes: int = 90               # temporal fill window\n",
    "    node_xy_crs: str = \"EPSG:3857\"          # CRS for node_xy (meters)\n",
    "    station_ll_crs: str = \"EPSG:4326\"       # CRS of station lon/lat columns\n",
    "    cache_dir: str = \"./climate_cache\"      # directory to store cached node-climate arrays\n",
    "    # column names in station file\n",
    "    station_id_col: str = \"SerialNum\"\n",
    "    station_lon_col: str = \"Xlon\"\n",
    "    station_lat_col: str = \"Ylat\"\n",
    "    # weather file columns\n",
    "    weather_id_col: str = \"serial\"\n",
    "    weather_time_col: str = \"datetime\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5c7142a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------- I/O & PREP -------------------------\n",
    "\n",
    "def _read_weather(cfg: ClimateConfig) -> pd.DataFrame:\n",
    "    df = pd.read_csv(cfg.weather_csv)\n",
    "    # unify station id name\n",
    "    if cfg.weather_id_col not in df.columns and \"SerialNum\" in df.columns:\n",
    "        df = df.rename(columns={\"SerialNum\": cfg.weather_id_col})\n",
    "    # parse time (tz-aware)\n",
    "    df[cfg.weather_time_col] = pd.to_datetime(df[cfg.weather_time_col], errors=\"coerce\", utc=True)\\\n",
    "                                   .dt.tz_convert(cfg.start_time_dt.tz)\n",
    "    df = df.dropna(subset=[cfg.weather_id_col, cfg.weather_time_col]).sort_values(\n",
    "        [cfg.weather_id_col, cfg.weather_time_col]\n",
    "    ).reset_index(drop=True)\n",
    "    # keep only present vars\n",
    "    cols_present = [c for c in cfg.var_cols if c in df.columns]\n",
    "    if not cols_present:\n",
    "        raise ValueError(\"None of cfg.var_cols found in weather CSV.\")\n",
    "    return df[[cfg.weather_id_col, cfg.weather_time_col] + cols_present].copy()\n",
    "\n",
    "\n",
    "def _read_stations(cfg: ClimateConfig) -> pd.DataFrame:\n",
    "    s = pd.read_csv(cfg.stations_csv)\n",
    "    if cfg.station_id_col not in s.columns:\n",
    "        raise ValueError(f\"Station id column '{cfg.station_id_col}' not in stations CSV.\")\n",
    "    # project lon/lat -> node CRS\n",
    "    transformer = Transformer.from_crs(cfg.station_ll_crs, cfg.node_xy_crs, always_xy=True)\n",
    "    x, y = transformer.transform(s[cfg.station_lon_col].values, s[cfg.station_lat_col].values)\n",
    "    out = pd.DataFrame({\n",
    "        \"serial\": s[cfg.station_id_col].astype(str),  # use str ids\n",
    "        \"sx\": x,\n",
    "        \"sy\": y,\n",
    "    })\n",
    "    return out\n",
    "\n",
    "\n",
    "def _hash_key(cfg: ClimateConfig, nodes_xy: np.ndarray) -> str:\n",
    "    h = hashlib.sha1()\n",
    "    h.update(str(cfg.start_time_dt.value).encode())\n",
    "    h.update(str(cfg.bin_seconds).encode())\n",
    "    h.update(\",\".join(sorted(cfg.var_cols)).encode())\n",
    "    h.update(str(nodes_xy.shape).encode())\n",
    "    h.update(str(int(nodes_xy.sum()*1e-9)).encode())  # weak checksum to detect different nodes\n",
    "    return h.hexdigest()[:12]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07638a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------- TEMPORAL GAP FILL -------------------------\n",
    "\n",
    "def _fill_short_gaps_linear(df: pd.DataFrame, cfg: ClimateConfig) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each station & feature:\n",
    "      - if current row has a value -> keep it\n",
    "      - else: if prev & next valid within max_gap_minutes -> linear interpolate in time\n",
    "              elif only one side within window -> use that side (nearest in time)\n",
    "              else -> remain NaN\n",
    "    Writes <feature>_filled columns; does NOT overwrite originals.\n",
    "    \"\"\"\n",
    "    df = df.sort_values([cfg.weather_id_col, cfg.weather_time_col]).copy()\n",
    "    t = df[cfg.weather_time_col]\n",
    "\n",
    "    for col in cfg.var_cols:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        vals = df[col]\n",
    "        g = df.groupby(cfg.weather_id_col)\n",
    "        prev_val  = g[col].ffill()\n",
    "        next_val  = g[col].bfill()\n",
    "        prev_time = t.where(vals.notna()).groupby(df[cfg.weather_id_col]).ffill()\n",
    "        next_time = t.where(vals.notna()).groupby(df[cfg.weather_id_col]).bfill()\n",
    "\n",
    "        dprev = (t - prev_time).dt.total_seconds() / 60.0\n",
    "        dnext = (next_time - t).dt.total_seconds() / 60.0\n",
    "\n",
    "        has_prev = prev_val.notna() & dprev.notna() & (dprev <= cfg.max_gap_minutes)\n",
    "        has_next = next_val.notna() & dnext.notna() & (dnext <= cfg.max_gap_minutes)\n",
    "\n",
    "        filled = vals.copy()\n",
    "\n",
    "        need = vals.isna()\n",
    "        both = need & has_prev & has_next\n",
    "        if both.any():\n",
    "            tot = dprev[both] + dnext[both]\n",
    "            w_prev = dnext[both] / tot\n",
    "            w_next = dprev[both] / tot\n",
    "            filled.loc[both] = w_prev * prev_val[both] + w_next * next_val[both]\n",
    "\n",
    "        only_prev = need & has_prev & ~has_next\n",
    "        if only_prev.any():\n",
    "            filled.loc[only_prev] = prev_val[only_prev]\n",
    "\n",
    "        only_next = need & has_next & ~has_prev\n",
    "        if only_next.any():\n",
    "            filled.loc[only_next] = next_val[only_next]\n",
    "\n",
    "        df[col + \"_filled\"] = filled\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ebdc100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------- BINNING -------------------------\n",
    "\n",
    "def _assign_bins(df: pd.DataFrame, cfg: ClimateConfig) -> pd.DataFrame:\n",
    "    tz_dt = cfg.start_time_dt if cfg.start_time_dt.tzinfo else cfg.start_time_dt.tz_localize(\"Asia/Seoul\")\n",
    "    t0 = int(tz_dt.timestamp())\n",
    "    secs = (df[cfg.weather_time_col].view(\"int64\") // 10**9) - t0\n",
    "    df = df.copy()\n",
    "    df[\"time_bin\"] = (secs // cfg.bin_seconds).astype(int)\n",
    "    return df\n",
    "\n",
    "\n",
    "def _aggregate_bins(df: pd.DataFrame, cfg: ClimateConfig) -> pd.DataFrame:\n",
    "    filled_cols = [c + \"_filled\" for c in cfg.var_cols if (c + \"_filled\") in df.columns]\n",
    "    if not filled_cols:\n",
    "        raise ValueError(\"No *_filled columns found. Did you run _fill_short_gaps_linear first?\")\n",
    "    grp = (df.groupby([cfg.weather_id_col, \"time_bin\"], as_index=False)[filled_cols]\n",
    "             .mean(numeric_only=True))\n",
    "    grp = grp.rename(columns={c: c.replace(\"_filled\", \"\") for c in filled_cols})\n",
    "    return grp  # columns: serial, time_bin, <features>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09fe401b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------- INTERPOLATION PER BIN -------------------------\n",
    "\n",
    "def _interpolate_bin(\n",
    "    stations_xy: pd.DataFrame,          # [\"serial\",\"sx\",\"sy\"]\n",
    "    bin_slice: pd.DataFrame,            # [\"serial\",\"time_bin\", features...], single bin\n",
    "    nodes_xy: np.ndarray,               # shape [N, 2]\n",
    "    feat_cols: List[str],\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns matrix [N_nodes, N_features] for this bin.\n",
    "    \"\"\"\n",
    "    # join station positions\n",
    "    df = bin_slice.merge(stations_xy, on=\"serial\", how=\"inner\")\n",
    "    out = np.full((nodes_xy.shape[0], len(feat_cols)), np.nan, dtype=float)\n",
    "\n",
    "    if df.empty:\n",
    "        return out\n",
    "\n",
    "    P = df[[\"sx\", \"sy\"]].to_numpy()\n",
    "\n",
    "    for j, col in enumerate(feat_cols):\n",
    "        z = df[col].to_numpy()\n",
    "        valid = np.isfinite(z)\n",
    "        if valid.sum() == 0:\n",
    "            continue\n",
    "        P_valid = P[valid]\n",
    "        z_valid = z[valid]\n",
    "\n",
    "        # if >=3 points → try linear on TIN; else → nearest\n",
    "        if P_valid.shape[0] >= 3:\n",
    "            lin = LinearNDInterpolator(P_valid, z_valid, rescale=False)  # builds its own Delaunay\n",
    "            vals = lin(nodes_xy)\n",
    "            # fill outside-convex-hull NaNs with nearest\n",
    "            if np.isnan(vals).any():\n",
    "                near = NearestNDInterpolator(P_valid, z_valid)\n",
    "                nanmask = np.isnan(vals)\n",
    "                vals[nanmask] = near(nodes_xy[nanmask])\n",
    "        else:\n",
    "            near = NearestNDInterpolator(P_valid, z_valid)\n",
    "            vals = near(nodes_xy)\n",
    "\n",
    "        out[:, j] = vals\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82da3d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------- PUBLIC API -------------------------\n",
    "\n",
    "def compute_node_climate_by_bin(\n",
    "    cfg: ClimateConfig,\n",
    "    nodes_xy: np.ndarray,           # shape [N_nodes, 2] in cfg.node_xy_crs\n",
    "    use_cache: bool = True,\n",
    "    force_recompute: bool = False,\n",
    ") -> Tuple[Dict[int, np.ndarray], List[str], str]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      - climate_by_bin: dict {time_bin: ndarray [N_nodes, N_features]}\n",
    "      - feature_names: same order as columns\n",
    "      - cache_path: file used/created (parquet)\n",
    "    \"\"\"\n",
    "    Path(cfg.cache_dir).mkdir(parents=True, exist_ok=True)\n",
    "    key = _hash_key(cfg, nodes_xy)\n",
    "    cache_path = os.path.join(cfg.cache_dir, f\"node_climate_bins_{key}.parquet\")\n",
    "\n",
    "    if use_cache and (not force_recompute) and os.path.exists(cache_path):\n",
    "        tbl = pd.read_parquet(cache_path)\n",
    "        # reconstruct dict\n",
    "        bins = sorted(tbl[\"time_bin\"].unique())\n",
    "        feat_cols = [c for c in tbl.columns if c not in (\"time_bin\", \"node_id\")]\n",
    "        n_nodes = tbl[\"node_id\"].max() + 1\n",
    "        climate_by_bin = {}\n",
    "        for tb in bins:\n",
    "            sub = tbl[tbl[\"time_bin\"] == tb].sort_values(\"node_id\")\n",
    "            M = sub[feat_cols].to_numpy()\n",
    "            # ensure shape [n_nodes, n_feats]\n",
    "            if M.shape[0] != n_nodes:\n",
    "                # sparse saved? fall back to re-compute\n",
    "                break\n",
    "            climate_by_bin[tb] = M\n",
    "        else:\n",
    "            return climate_by_bin, feat_cols, cache_path\n",
    "        # fallthrough to recompute if any inconsistency\n",
    "\n",
    "    # load + prep\n",
    "    weather = _read_weather(cfg)\n",
    "    stations = _read_stations(cfg)\n",
    "    # unify id types\n",
    "    weather[cfg.weather_id_col] = weather[cfg.weather_id_col].astype(str)\n",
    "\n",
    "    # temporal fill\n",
    "    weather_filled = _fill_short_gaps_linear(weather, cfg)\n",
    "\n",
    "    # binning & aggregation\n",
    "    weather_filled = _assign_bins(weather_filled, cfg)\n",
    "    agg = _aggregate_bins(weather_filled, cfg)  # serial, time_bin, features\n",
    "\n",
    "    # build per-bin node climate\n",
    "    bins = sorted(agg[\"time_bin\"].unique())\n",
    "    feat_cols = [c for c in cfg.var_cols if c in agg.columns]\n",
    "    climate_by_bin: Dict[int, np.ndarray] = {}\n",
    "\n",
    "    for tb in bins:\n",
    "        slice_tb = agg[agg[\"time_bin\"] == tb][[cfg.weather_id_col, \"time_bin\"] + feat_cols]\\\n",
    "                        .rename(columns={cfg.weather_id_col: \"serial\"})\n",
    "        climate_by_bin[tb] = _interpolate_bin(stations, slice_tb, nodes_xy, feat_cols)\n",
    "\n",
    "    # save cache as long table: (time_bin, node_id, features...)\n",
    "    rows = []\n",
    "    for tb, M in climate_by_bin.items():\n",
    "        n_nodes, n_feats = M.shape\n",
    "        df_row = pd.DataFrame({\n",
    "            \"time_bin\": np.full(n_nodes, tb, dtype=int),\n",
    "            \"node_id\": np.arange(n_nodes, dtype=int),\n",
    "        })\n",
    "        for j, col in enumerate(feat_cols):\n",
    "            df_row[col] = M[:, j]\n",
    "        rows.append(df_row)\n",
    "    cache_tbl = pd.concat(rows, ignore_index=True)\n",
    "    # write with minimal metadata\n",
    "    cache_tbl.attrs[\"config\"] = json.dumps({\n",
    "        \"bin_seconds\": cfg.bin_seconds,\n",
    "        \"start_time\": str(cfg.start_time_dt),\n",
    "        \"var_cols\": feat_cols,\n",
    "        \"node_xy_crs\": cfg.node_xy_crs,\n",
    "        \"hash_key\": key,\n",
    "    })\n",
    "    cache_tbl.to_parquet(cache_path, index=False)\n",
    "\n",
    "    return climate_by_bin, feat_cols, cache_path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venvth)",
   "language": "python",
   "name": "venvth"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
