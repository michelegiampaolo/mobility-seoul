{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1c8a4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "12.1\n",
      "NVIDIA GeForce GTX 960M\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Optional, Tuple, Dict, Iterable\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda if torch.version else \"no torch.version\")\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0110e6",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd9021e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    # Data paths\n",
    "    \"base_dir\": \"D:/Desktop/MSc Thesis - Copy/\",\n",
    "    \"nodes_csv\": \"shapefiles/_network_parts_4_intersections/node_urban_features.csv\",\n",
    "    \"links_csv\": \"shapefiles/_network_parts_4_intersections/positions_links_detailed.csv\",\n",
    "\n",
    "    # Urban features\n",
    "    \"using_urban_features\": True,\n",
    "    \"urban_features_to_use\": [\"GSI\", \"WMHB\", \"GrCR\", \"GD\"],\n",
    "    \"include_urban_in_node_features\": False,\n",
    "\n",
    "    # Filtering\n",
    "    \"min_participants_per_day\": 10,\n",
    "    \"tz\": \"Asia/Seoul\",\n",
    "\n",
    "    # Binning\n",
    "    \"bin_hours\": 6,                 # choose from 1,2,3,4,6,8,12,24\n",
    "    \"align_to_midnight\": True,\n",
    "\n",
    "    # Model + training\n",
    "    \"window_size\": 4,\n",
    "    \"embedding_dim\": 64,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"num_epochs\": 60,\n",
    "    \"encoder_edge_feat_dim\": 4,     # hour, day, flow, transpor\n",
    "    \"time_feat_dim\": 2,             # hour_norm, day_norm\n",
    "    \"use_mode_loss\": False,\n",
    "\n",
    "    # Loss normalizers (you were using 1s)\n",
    "    \"manual_max_losses\": {\n",
    "        \"node_count\": 1.0,\n",
    "        \"node_presence\": 1.0,\n",
    "        \"edge_flow\": 1.0,\n",
    "        \"transport_mode\": 1.0\n",
    "    },\n",
    "\n",
    "    # Presence loss: currently setting it to 0\n",
    "    \"use_presence_loss\": False,\n",
    "    \"presence_threshold\": 0.5,      # for active node thresholding in edge candidates\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864e21bb",
   "metadata": {},
   "source": [
    "# General functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f3fabce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def load_csvs(params: Dict) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    nodes_df = pd.read_csv(os.path.join(params[\"base_dir\"], params[\"nodes_csv\"]), header=0)\n",
    "    links_df = pd.read_csv(os.path.join(params[\"base_dir\"], params[\"links_csv\"]), header=0)\n",
    "    return nodes_df, links_df\n",
    "\n",
    "def reindex_node_ids(nodes_df: pd.DataFrame, links_df: pd.DataFrame):\n",
    "    original_ids = sorted(nodes_df[\"node_id\"].unique())\n",
    "    id_map = {old: new for new, old in enumerate(original_ids)}\n",
    "    nodes_df = nodes_df.copy()\n",
    "    links_df = links_df.copy()\n",
    "    nodes_df[\"node_id_original\"] = nodes_df[\"node_id\"]\n",
    "    nodes_df[\"node_id\"] = nodes_df[\"node_id\"].map(id_map)\n",
    "    links_df[\"node_id\"] = links_df[\"node_id\"].map(id_map)\n",
    "    links_df[\"prev_node_id\"] = links_df[\"prev_node_id\"].map(id_map)\n",
    "    return nodes_df, links_df\n",
    "\n",
    "def prepare_inputs(params: Dict):\n",
    "    nodes_df, links_df = load_csvs(params)\n",
    "\n",
    "    # drop geometry and cast bools to ints\n",
    "    if \"geometry\" in nodes_df.columns:\n",
    "        nodes_df = nodes_df.drop(columns=[\"geometry\"])\n",
    "    for col in nodes_df.select_dtypes(include=[\"bool\"]).columns:\n",
    "        nodes_df[col] = nodes_df[col].astype(int)\n",
    "\n",
    "    # timestamps & ids\n",
    "    links_df[\"timestamp\"] = pd.to_datetime(links_df[\"merged_datetime\"], format='ISO8601')\n",
    "    links_df[\"merged_datetime\"] = pd.to_datetime(links_df[\"merged_datetime\"], format='ISO8601').astype(int) // 10**9\n",
    "    links_df[\"node_id\"] = links_df[\"node_id\"].astype(int)\n",
    "    links_df[\"prev_node_id\"] = links_df[\"prev_node_id\"].astype(int)\n",
    "    links_df[\"id_participant\"] = links_df[\"id_participant\"].astype(int)\n",
    "    if links_df[\"id_participant\"].min() > 0:\n",
    "        links_df[\"id_participant\"] = links_df[\"id_participant\"] - 1\n",
    "\n",
    "    links_df[\"hour_of_day\"] = links_df[\"timestamp\"].dt.hour\n",
    "    links_df[\"day_of_week\"] = links_df[\"timestamp\"].dt.dayofweek\n",
    "\n",
    "    # reindex nodes to [0..N-1]\n",
    "    nodes_df, links_df = reindex_node_ids(nodes_df, links_df)\n",
    "\n",
    "    # filter days with enough participants\n",
    "    links_df[\"timestamp\"] = pd.to_datetime(links_df[\"timestamp\"], utc=True).dt.tz_convert(params[\"tz\"])\n",
    "    links_df[\"day\"] = links_df[\"timestamp\"].dt.floor(\"D\")\n",
    "    counts = links_df.groupby(\"day\")[\"id_participant\"].nunique()\n",
    "    active_days = counts[counts >= params[\"min_participants_per_day\"]].index\n",
    "    links_df = links_df[links_df[\"day\"].isin(active_days)].copy()\n",
    "\n",
    "    return nodes_df, links_df\n",
    "\n",
    "def normalize_urban(nodes_df: pd.DataFrame, feat_names: List[str]) -> np.ndarray:\n",
    "    if not feat_names:\n",
    "        return np.empty((len(nodes_df), 0))\n",
    "    X = nodes_df[feat_names]\n",
    "    Xn = (X - X.min()) / (X.max() - X.min())\n",
    "    # fill rules you used\n",
    "    fill_values = {\"GSI\": 0.0, \"FSI\": 0.0, \"OSR\": 1.0, \"ABH\": 0.0, \"WMHB\": 0.0, \"HSTD\": 0.0, \"GrCR\": 0.0, \"GD\": 1.0}\n",
    "    for c in feat_names:\n",
    "        if c in fill_values:\n",
    "            Xn[c] = Xn[c].fillna(fill_values[c])\n",
    "    return Xn.values\n",
    "\n",
    "def make_node_features(nodes_df: pd.DataFrame, params: Dict) -> torch.Tensor:\n",
    "    coords = nodes_df[[\"node_x\", \"node_y\"]].values\n",
    "    cmin, cmax = coords.min(axis=0), coords.max(axis=0)\n",
    "    denom = np.where((cmax - cmin) == 0, 1, (cmax - cmin))\n",
    "    coords_norm = (coords - cmin) / denom\n",
    "\n",
    "    if params[\"using_urban_features\"] and params[\"include_urban_in_node_features\"]:\n",
    "        uf = normalize_urban(nodes_df, params[\"urban_features_to_use\"])\n",
    "        arr = np.hstack([coords_norm, uf])\n",
    "    else:\n",
    "        #(coords only)\n",
    "        arr = coords_norm\n",
    "\n",
    "    return torch.tensor(arr, dtype=torch.float)\n",
    "\n",
    "def bin_links(links_df: pd.DataFrame, params: Dict):\n",
    "    df = links_df.copy()\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], utc=True).dt.tz_convert(params[\"tz\"])\n",
    "\n",
    "    if params[\"align_to_midnight\"]:\n",
    "        start_dt = df[\"timestamp\"].dt.floor(\"D\").min()\n",
    "    else:\n",
    "        start_dt = df[\"timestamp\"].min().floor(f'{params[\"bin_hours\"]}H')\n",
    "\n",
    "    interval = params[\"bin_hours\"] * 3600\n",
    "    start_sec = int(start_dt.timestamp())\n",
    "    df[\"time_seconds\"] = df[\"merged_datetime\"] - start_sec\n",
    "    df[\"time_bin\"] = (df[\"time_seconds\"] // interval).astype(int)\n",
    "\n",
    "    # bin context (hour_norm, day_norm) from bin midpoint\n",
    "    min_bin, max_bin = int(df[\"time_bin\"].min()), int(df[\"time_bin\"].max())\n",
    "    bin_context = {}\n",
    "    for t in range(min_bin, max_bin + 1):\n",
    "        bin_start = start_dt + pd.Timedelta(seconds=interval * t)\n",
    "        mid = bin_start + pd.Timedelta(seconds=interval // 2)\n",
    "        hour_norm = (mid.hour + mid.minute/60) / 23.0\n",
    "        day_norm = mid.dayofweek / 6.0\n",
    "        bin_context[t] = (hour_norm, day_norm)\n",
    "\n",
    "    return df, bin_context, range(min_bin, max_bin + 1)\n",
    "\n",
    "def classify_transport_mode(speed_norm, speed_min, speed_max):\n",
    "    speed = speed_norm * (speed_max - speed_min) + speed_min\n",
    "    return 0 if speed <= 7.5 else 1\n",
    "\n",
    "def build_snapshots(nodes_df, links_df, node_features: torch.Tensor, params: Dict):\n",
    "    df = links_df.copy()\n",
    "    df[\"hour_norm\"] = df[\"hour_of_day\"] / 23.0\n",
    "    df[\"day_norm\"] = df[\"day_of_week\"] / 6.0\n",
    "    sp_min, sp_max = df[\"speed\"].min(), df[\"speed\"].max()\n",
    "    df[\"speed_norm\"] = (df[\"speed\"] - sp_min) / (sp_max - sp_min)\n",
    "\n",
    "    df, bin_context, all_bins = bin_links(df, params)\n",
    "    num_nodes = nodes_df[\"node_id\"].nunique()\n",
    "\n",
    "    features, edge_indices, edge_feats, targets = [], [], [], []\n",
    "    for t in all_bins:\n",
    "        now = df[df[\"time_bin\"] == t].copy()\n",
    "        nxt = df[df[\"time_bin\"] == t + 1].copy()\n",
    "\n",
    "        # node features: [visit_count] + node_features\n",
    "        visit = now.groupby(\"node_id\").size().reindex(range(num_nodes), fill_value=0).values\n",
    "        visit_t = torch.tensor(visit, dtype=torch.float).unsqueeze(1)\n",
    "        x = torch.cat([visit_t, node_features], dim=1)\n",
    "        features.append(x)\n",
    "\n",
    "        # edge features this step\n",
    "        if not now.empty:\n",
    "            g = now.groupby([\"prev_node_id\", \"node_id\"]).agg({\n",
    "                \"hour_norm\": \"mean\",\n",
    "                \"day_norm\": \"mean\",\n",
    "                \"speed_norm\": \"mean\",\n",
    "                \"id_participant\": \"count\"  # flow\n",
    "            }).rename(columns={\"id_participant\": \"flow\"}).reset_index()\n",
    "            g[\"transport\"] = g[\"speed_norm\"].apply(lambda s: classify_transport_mode(s, sp_min, sp_max))\n",
    "\n",
    "            src = torch.tensor(g[\"prev_node_id\"].values, dtype=torch.long)\n",
    "            dst = torch.tensor(g[\"node_id\"].values, dtype=torch.long)\n",
    "            ei = torch.stack([src, dst], dim=0)\n",
    "            ef = torch.tensor(g[[\"hour_norm\", \"day_norm\", \"flow\", \"transport\"]].values, dtype=torch.float)\n",
    "        else:\n",
    "            ei = torch.empty((2, 0), dtype=torch.long)\n",
    "            ef = torch.empty((0, 4), dtype=torch.float)\n",
    "\n",
    "        edge_indices.append(ei)\n",
    "        edge_feats.append(ef)\n",
    "\n",
    "        # targets @ t+1\n",
    "        if nxt.empty:\n",
    "            node_target = torch.zeros(num_nodes)\n",
    "            next_ei = torch.empty((2, 0), dtype=torch.long)\n",
    "            flow_target = torch.empty((0,))\n",
    "            mode_target = torch.empty((0,), dtype=torch.long)\n",
    "        else:\n",
    "            vt = nxt.groupby(\"node_id\").size().reindex(range(num_nodes), fill_value=0).values\n",
    "            node_target = torch.tensor(vt, dtype=torch.float)\n",
    "\n",
    "            gn = nxt.groupby([\"prev_node_id\", \"node_id\"]).agg({\n",
    "                \"speed_norm\": \"mean\",\n",
    "                \"hour_norm\": \"mean\",\n",
    "                \"day_norm\": \"mean\",\n",
    "                \"id_participant\": \"count\"\n",
    "            }).rename(columns={\"id_participant\": \"flow\"}).reset_index()\n",
    "            gn[\"transport\"] = gn[\"speed_norm\"].apply(lambda s: classify_transport_mode(s, sp_min, sp_max))\n",
    "\n",
    "            ns = torch.tensor(gn[\"prev_node_id\"].values, dtype=torch.long)\n",
    "            nd = torch.tensor(gn[\"node_id\"].values, dtype=torch.long)\n",
    "            next_ei = torch.stack([ns, nd], dim=0)\n",
    "            flow_target = torch.tensor(gn[\"speed_norm\"].values, dtype=torch.float)\n",
    "            mode_target = torch.tensor(gn[\"transport\"].values, dtype=torch.long)\n",
    "\n",
    "        targets.append((node_target, next_ei, flow_target, mode_target))\n",
    "\n",
    "    return features, edge_indices, edge_feats, targets, bin_context, all_bins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101cb77d",
   "metadata": {},
   "source": [
    "# Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45f18a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import NNConv\n",
    "from torch.nn import GRUCell, Linear, ReLU, Sequential\n",
    "\n",
    "class EdgeAwareTemporalEncoder(nn.Module):\n",
    "    def __init__(self, in_channels, edge_feat_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.edge_nn = Sequential(\n",
    "            Linear(edge_feat_dim, 32),\n",
    "            ReLU(),\n",
    "            Linear(32, in_channels * hidden_dim)\n",
    "        )\n",
    "        self.conv = NNConv(in_channels, hidden_dim, nn=self.edge_nn, aggr='mean')\n",
    "        self.gru = GRUCell(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x_seq, edge_index_seq, edge_attr_seq):\n",
    "        h = None\n",
    "        for x, edge_index, edge_attr in zip(x_seq, edge_index_seq, edge_attr_seq):\n",
    "            out = self.conv(x, edge_index, edge_attr)\n",
    "            h = out if h is None else self.gru(out, h)\n",
    "        return h\n",
    "\n",
    "class MovementLinkPredictor(nn.Module):\n",
    "    def __init__(self, in_channels, encoder_edge_feat_dim, time_feat_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = EdgeAwareTemporalEncoder(in_channels, encoder_edge_feat_dim, hidden_dim)\n",
    "        self.node_decoder = nn.Sequential(Linear(hidden_dim + time_feat_dim, 64), ReLU(), Linear(64, 1))\n",
    "        self.edge_flow_decoder = nn.Sequential(nn.Linear(2*hidden_dim + time_feat_dim, 64), nn.ReLU(), nn.Linear(64, 1))\n",
    "        self.transport_decoder = nn.Sequential(nn.Linear(2*hidden_dim + time_feat_dim, 64), nn.ReLU(), nn.Linear(64, 2))\n",
    "\n",
    "    def forward_encoder(self, x_seq, edge_index_seq, edge_attr_seq):\n",
    "        return self.encoder(x_seq, edge_index_seq, edge_attr_seq)\n",
    "\n",
    "    def predict_node_counts(self, z, time_feat):\n",
    "        return self.node_decoder(torch.cat([z, time_feat], dim=1)).squeeze(1)\n",
    "\n",
    "    def predict_edge_flow(self, z, edge_index, time_feat):\n",
    "        src = z[edge_index[0]]\n",
    "        dst = z[edge_index[1]]\n",
    "        return self.edge_flow_decoder(torch.cat([src, dst, time_feat], dim=1)).squeeze(1)\n",
    "\n",
    "    def predict_edge_mode(self, z, edge_index, time_feat):\n",
    "        src = z[edge_index[0]]\n",
    "        dst = z[edge_index[1]]\n",
    "        return self.transport_decoder(torch.cat([src, dst, time_feat], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5380a82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_time_bins_by_day_cadence(df: pd.DataFrame, window_size: int):\n",
    "    \"\"\"\n",
    "    Keep your cadence idea but short:\n",
    "    - Tag days as train/val/test in repeating blocks (8T/4V/6T/2Te).\n",
    "    - Remove target bins whose input window crosses groups.\n",
    "    \"\"\"\n",
    "    d = df.copy()\n",
    "    min_day = d[\"day\"].min()\n",
    "    d[\"day_index\"] = (d[\"day\"] - min_day).dt.days\n",
    "    unique_days = sorted(d[\"day_index\"].unique())\n",
    "\n",
    "    pattern = ([\"train\"]*8) + ([\"val\"]*4) + ([\"train\"]*6) + ([\"test\"]*2)\n",
    "    L = len(pattern)\n",
    "\n",
    "    day_to_group = {}\n",
    "    for i, day in enumerate(unique_days):\n",
    "        day_to_group[day] = pattern[i % L]\n",
    "\n",
    "    d[\"group\"] = d[\"day_index\"].map(day_to_group)\n",
    "    bin_groups = d.groupby(\"time_bin\")[\"group\"].agg(lambda x: x.value_counts().idxmax())\n",
    "\n",
    "    valid_bins = []\n",
    "    for t in bin_groups.index:\n",
    "        g = bin_groups[t]\n",
    "        ok = True\n",
    "        for b in range(t - window_size, t):\n",
    "            if b not in bin_groups or bin_groups[b] != g:\n",
    "                ok = False\n",
    "                break\n",
    "        if ok:\n",
    "            valid_bins.append(t)\n",
    "\n",
    "    train_bins = [t for t in valid_bins if bin_groups[t] == \"train\"]\n",
    "    val_bins   = [t for t in valid_bins if bin_groups[t] == \"val\"]\n",
    "    test_bins  = [t for t in valid_bins if bin_groups[t] == \"test\"]\n",
    "    return train_bins, val_bins, test_bins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a062b19",
   "metadata": {},
   "source": [
    "# Model training definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d5d6c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------ SHARED FORWARD STEP ----------------------------------\n",
    "def forward_step(model, t, links_df, features, edge_indices, edge_feats, targets, bin_context, params):\n",
    "    # Window slices\n",
    "    ws = params[\"window_size\"]\n",
    "    x_seq = features[t - ws:t]\n",
    "    ei_seq = edge_indices[t - ws:t]\n",
    "    ef_seq = edge_feats[t - ws:t]\n",
    "    if all(ei.size(1) == 0 for ei in ei_seq):\n",
    "        return None  # skip\n",
    "\n",
    "    z = model.forward_encoder(x_seq, ei_seq, ef_seq)\n",
    "\n",
    "    hour, day = bin_context[t]\n",
    "    time_feat = torch.tensor([[hour, day]], dtype=torch.float)\n",
    "    num_nodes = features[0].shape[0]\n",
    "    node_time_feat = time_feat.repeat(num_nodes, 1)\n",
    "\n",
    "    node_target, next_ei, flow_target, mode_target = targets[t - 1]\n",
    "    node_pred = model.predict_node_counts(z, node_time_feat)\n",
    "\n",
    "    # Presence (your current training disables it)\n",
    "    if params[\"use_presence_loss\"]:\n",
    "        # linear map 0..inf → clamp 0..1\n",
    "        p_active = torch.clamp(node_pred, 0.0, 1.0)\n",
    "        is_active = (node_target >= 1).float()\n",
    "        presence_loss = F.binary_cross_entropy(p_active, is_active)\n",
    "    else:\n",
    "        presence_loss = torch.tensor(0.0)\n",
    "\n",
    "    mse_loss = F.mse_loss(node_pred, node_target)\n",
    "\n",
    "    # Candidate edges\n",
    "    threshold = params[\"presence_threshold\"]\n",
    "    active_nodes = (node_pred >= threshold).nonzero(as_tuple=False).flatten()\n",
    "    edge_loss = torch.tensor(0.0)\n",
    "    mode_loss = torch.tensor(0.0)\n",
    "\n",
    "    if active_nodes.numel() > 0:\n",
    "        df_prev = links_df[links_df[\"time_bin\"] == t - 1]\n",
    "        last_pos = df_prev.sort_values(\"merged_datetime\").groupby(\"id_participant\").tail(1)\n",
    "        src_nodes = torch.tensor(last_pos[\"node_id\"].unique(), dtype=torch.long)\n",
    "\n",
    "        # source→active + active→active\n",
    "        from itertools import product\n",
    "        pairs = list(set(product(src_nodes.tolist(), active_nodes.tolist())) |\n",
    "                     set(product(active_nodes.tolist(), active_nodes.tolist())))\n",
    "\n",
    "        if len(pairs) > 0:\n",
    "            cand = torch.tensor(pairs, dtype=torch.long).T  # [2, E]\n",
    "            # true lookup\n",
    "            true_ei = next_ei\n",
    "            true_flow = flow_target\n",
    "            true_mode = mode_target\n",
    "            true_map = {tuple(e): i for i, e in enumerate(true_ei.T.tolist())}\n",
    "\n",
    "            flow_targets = torch.tensor(\n",
    "                [true_flow[true_map[(int(s), int(d))]] if (int(s), int(d)) in true_map else 0.0\n",
    "                 for s, d in cand.T], dtype=torch.float\n",
    "            )\n",
    "            edge_time_feat = time_feat.repeat(cand.shape[1], 1)\n",
    "            pred_flow = model.predict_edge_flow(z, cand, edge_time_feat)\n",
    "\n",
    "            # weight positives higher\n",
    "            weights = torch.where(flow_targets > 0, torch.tensor(10.0), torch.tensor(1.0))\n",
    "            edge_loss = F.mse_loss(pred_flow, flow_targets, reduction='none')\n",
    "            edge_loss = (edge_loss * weights).mean()\n",
    "\n",
    "            if params[\"use_mode_loss\"]:\n",
    "                exists = torch.tensor([(int(s), int(d)) in true_map for s, d in cand.T], dtype=torch.bool)\n",
    "                if exists.any():\n",
    "                    valid = cand[:, exists]\n",
    "                    m_targets = torch.tensor([true_mode[true_map[(int(s), int(d))]] for s, d in valid.T],\n",
    "                                             dtype=torch.long)\n",
    "                    m_logits = model.predict_edge_mode(z, valid, time_feat.repeat(valid.shape[1], 1))\n",
    "                    mode_loss = F.cross_entropy(m_logits, m_targets)\n",
    "                else:\n",
    "                    mode_loss = torch.tensor(0.0)\n",
    "\n",
    "    # Normalize pieces (you used all 1.0 by default)\n",
    "    mm = params[\"manual_max_losses\"]\n",
    "    total = (mse_loss/mm[\"node_count\"]) + (presence_loss/mm[\"node_presence\"]) + (edge_loss/mm[\"edge_flow\"])\n",
    "    if params[\"use_mode_loss\"]:\n",
    "        total = total + (mode_loss/mm[\"transport_mode\"])\n",
    "\n",
    "    return total, mse_loss, presence_loss, edge_loss, mode_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36e0fb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------ TRAIN / VAL LOOPS ------------------------------------\n",
    "def compute_val(model, params, links_df, features, edge_indices, edge_feats, targets, bin_context, bins):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        s = 0\n",
    "        acc = np.zeros(5, dtype=float)\n",
    "        for t in bins:\n",
    "            out = forward_step(model, t, links_df, features, edge_indices, edge_feats, targets, bin_context, params)\n",
    "            if out is None:\n",
    "                continue\n",
    "            vals = [float(x.item()) for x in out]\n",
    "            acc += np.array(vals)\n",
    "            s += 1\n",
    "        if s == 0:\n",
    "            return [float('nan')]*5\n",
    "        return (acc / s).tolist()\n",
    "\n",
    "def train_one_run(params: Dict):\n",
    "    set_seed(42)\n",
    "\n",
    "    # 1) Data\n",
    "    nodes_df, links_df = prepare_inputs(params)\n",
    "    node_features = make_node_features(nodes_df, params)\n",
    "    features, edge_indices, edge_feats, targets, bin_context, all_bins = build_snapshots(\n",
    "        nodes_df, links_df, node_features, params\n",
    "    )\n",
    "\n",
    "    # Splits\n",
    "    train_bins, val_bins, test_bins = assign_time_bins_by_day_cadence(links_df, params[\"window_size\"])\n",
    "\n",
    "    # 2) Model\n",
    "    model = MovementLinkPredictor(\n",
    "        in_channels=features[0].shape[1],\n",
    "        encoder_edge_feat_dim=params[\"encoder_edge_feat_dim\"],\n",
    "        time_feat_dim=params[\"time_feat_dim\"],\n",
    "        hidden_dim=params[\"embedding_dim\"]\n",
    "    )\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=params[\"learning_rate\"])\n",
    "    sch = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min', factor=0.5, patience=5, threshold=0.0005, verbose=True)\n",
    "\n",
    "    # 3) Train\n",
    "    train_hist = {\"total\": [], \"mse\": [], \"presence\": [], \"edge\": [], \"mode\": []}\n",
    "    val_hist   = {\"total\": [], \"mse\": [], \"presence\": [], \"edge\": [], \"mode\": []}\n",
    "\n",
    "    for epoch in range(params[\"num_epochs\"]):\n",
    "        model.train()\n",
    "        s = 0\n",
    "        acc = np.zeros(5, dtype=float)\n",
    "        for t in train_bins:\n",
    "            out = forward_step(model, t, links_df, features, edge_indices, edge_feats, targets, bin_context, params)\n",
    "            if out is None:\n",
    "                continue\n",
    "            total_loss, mse_loss, presence_loss, edge_loss, mode_loss = out\n",
    "            opt.zero_grad()\n",
    "            total_loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            vals = [float(x.item()) for x in [total_loss, mse_loss, presence_loss, edge_loss, mode_loss]]\n",
    "            acc += np.array(vals)\n",
    "            s += 1\n",
    "\n",
    "        train_vals = (acc / s).tolist() if s else [float('nan')]*5\n",
    "        for k, v in zip(train_hist.keys(), train_vals):\n",
    "            train_hist[k].append(v)\n",
    "\n",
    "        # validation with SAME computation\n",
    "        v_tot, v_mse, v_pres, v_edge, v_mode = compute_val(\n",
    "            model, params, links_df, features, edge_indices, edge_feats, targets, bin_context, val_bins\n",
    "        )\n",
    "        for k, v in zip(val_hist.keys(), [v_tot, v_mse, v_pres, v_edge, v_mode]):\n",
    "            val_hist[k].append(v)\n",
    "\n",
    "        sch.step(train_hist[\"total\"][-1])\n",
    "        print(f\"Epoch {epoch+1}/{params['num_epochs']} | Train {train_hist['total'][-1]:.4f} | Val {val_hist['total'][-1]:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"features\": features,\n",
    "        \"edge_indices\": edge_indices,\n",
    "        \"edge_feats\": edge_feats,\n",
    "        \"targets\": targets,\n",
    "        \"bin_context\": bin_context,\n",
    "        \"splits\": (train_bins, val_bins, test_bins),\n",
    "        \"train_hist\": train_hist,\n",
    "        \"val_hist\": val_hist,\n",
    "        \"links_df\": links_df,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0635f311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------ MULTI-RUN SWEEP --------------------------------------\n",
    "def sweep(param_list: List[Dict]):\n",
    "    results = []\n",
    "    for i, p in enumerate(param_list, 1):\n",
    "        print(\"\\n\" + \"=\"*20 + f\" RUN {i} \" + \"=\"*20)\n",
    "        res = train_one_run(p)\n",
    "        results.append((p, res))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd52c04",
   "metadata": {},
   "source": [
    "# Running the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d761ef16",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'time_bin'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m      2\u001b[39m     \u001b[38;5;66;03m# Single run:\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     result = \u001b[43mtrain_one_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPARAMS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Or: multiple runs with small tweaks\u001b[39;00m\n\u001b[32m      6\u001b[39m     \u001b[38;5;66;03m# runs = [\u001b[39;00m\n\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m#     {**PARAMS, \"bin_hours\": 6, \"embedding_dim\": 64, \"use_presence_loss\": False, \"use_mode_loss\": False},\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m# ]\u001b[39;00m\n\u001b[32m     11\u001b[39m     \u001b[38;5;66;03m# all_results = sweep(runs)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mtrain_one_run\u001b[39m\u001b[34m(params)\u001b[39m\n\u001b[32m     24\u001b[39m features, edge_indices, edge_feats, targets, bin_context, all_bins = build_snapshots(\n\u001b[32m     25\u001b[39m     nodes_df, links_df, node_features, params\n\u001b[32m     26\u001b[39m )\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Splits\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m train_bins, val_bins, test_bins = \u001b[43massign_time_bins_by_day_cadence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlinks_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwindow_size\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# 2) Model\u001b[39;00m\n\u001b[32m     32\u001b[39m model = MovementLinkPredictor(\n\u001b[32m     33\u001b[39m     in_channels=features[\u001b[32m0\u001b[39m].shape[\u001b[32m1\u001b[39m],\n\u001b[32m     34\u001b[39m     encoder_edge_feat_dim=params[\u001b[33m\"\u001b[39m\u001b[33mencoder_edge_feat_dim\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     35\u001b[39m     time_feat_dim=params[\u001b[33m\"\u001b[39m\u001b[33mtime_feat_dim\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     36\u001b[39m     hidden_dim=params[\u001b[33m\"\u001b[39m\u001b[33membedding_dim\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     37\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36massign_time_bins_by_day_cadence\u001b[39m\u001b[34m(df, window_size)\u001b[39m\n\u001b[32m     17\u001b[39m     day_to_group[day] = pattern[i % L]\n\u001b[32m     19\u001b[39m d[\u001b[33m\"\u001b[39m\u001b[33mgroup\u001b[39m\u001b[33m\"\u001b[39m] = d[\u001b[33m\"\u001b[39m\u001b[33mday_index\u001b[39m\u001b[33m\"\u001b[39m].map(day_to_group)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m bin_groups = \u001b[43md\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtime_bin\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mgroup\u001b[39m\u001b[33m\"\u001b[39m].agg(\u001b[38;5;28;01mlambda\u001b[39;00m x: x.value_counts().idxmax())\n\u001b[32m     22\u001b[39m valid_bins = []\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m bin_groups.index:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Michele\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:9183\u001b[39m, in \u001b[36mDataFrame.groupby\u001b[39m\u001b[34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[39m\n\u001b[32m   9180\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   9181\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mYou have to supply one of \u001b[39m\u001b[33m'\u001b[39m\u001b[33mby\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlevel\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m9183\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   9184\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   9185\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9186\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9187\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9188\u001b[39m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9189\u001b[39m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9190\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9191\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9192\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9193\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Michele\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1329\u001b[39m, in \u001b[36mGroupBy.__init__\u001b[39m\u001b[34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[39m\n\u001b[32m   1326\u001b[39m \u001b[38;5;28mself\u001b[39m.dropna = dropna\n\u001b[32m   1328\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1329\u001b[39m     grouper, exclusions, obj = \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1332\u001b[39m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1334\u001b[39m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1335\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mno_default\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1336\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1337\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1339\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib.no_default:\n\u001b[32m   1340\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ping._passed_categorical \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m grouper.groupings):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Michele\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\groupby\\grouper.py:1043\u001b[39m, in \u001b[36mget_grouper\u001b[39m\u001b[34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[39m\n\u001b[32m   1041\u001b[39m         in_axis, level, gpr = \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1042\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1043\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[32m   1044\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr.key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1045\u001b[39m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[32m   1046\u001b[39m     exclusions.add(gpr.key)\n",
      "\u001b[31mKeyError\u001b[39m: 'time_bin'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Single run:\n",
    "    result = train_one_run(PARAMS)\n",
    "\n",
    "    # Or: multiple runs with small tweaks\n",
    "    # runs = [\n",
    "    #     {**PARAMS, \"bin_hours\": 6, \"embedding_dim\": 64, \"use_presence_loss\": False, \"use_mode_loss\": False},\n",
    "    #     {**PARAMS, \"bin_hours\": 3, \"embedding_dim\": 128, \"use_presence_loss\": False, \"use_mode_loss\": False},\n",
    "    #     {**PARAMS, \"include_urban_in_node_features\": True},  # try adding urban features into nodes\n",
    "    # ]\n",
    "    # all_results = sweep(runs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venvth)",
   "language": "python",
   "name": "venvth"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
